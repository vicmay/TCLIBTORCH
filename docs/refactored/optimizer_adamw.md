# torch::optimizer_adamw\n\nCreates an AdamW optimizer with support for both legacy positional and modern named parameter syntax.\n\n## Syntax\n\n### Named Parameter Syntax (Recommended)\n```tcl\ntorch::optimizer_adamw -parameters $paramList -lr $learningRate ?-beta1 $beta1? ?-beta2 $beta2? ?-eps $eps? ?-weightDecay $weightDecay? ?-amsgrad $amsgrad?\ntorch::optimizerAdamW -parameters $paramList -lr $learningRate ?-beta1 $beta1? ?-beta2 $beta2? ?-eps $eps? ?-weightDecay $weightDecay? ?-amsgrad $amsgrad?\n```\n\n### Legacy Positional Syntax (Backward Compatibility)\n```tcl\ntorch::optimizer_adamw $paramList $learningRate ?$weightDecay?\n```\n\n## Parameters\n\n### Named Parameters\n- **-parameters** | **-params** (required): List of tensor handles representing model parameters\n- **-lr** | **-learningRate** (required): Learning rate (positive float)\n- **-beta1** (optional): First moment decay rate (default: 0.9, range: [0,1))\n- **-beta2** (optional): Second moment decay rate (default: 0.999, range: [0,1))\n- **-eps** | **-epsilon** (optional): Epsilon for numerical stability (default: 1e-8, positive)\n- **-weightDecay** | **-weight_decay** (optional): Weight decay coefficient (default: 0.01, non-negative)\n- **-amsgrad** (optional): Whether to use AMSGrad variant (default: false, boolean)\n\n### Positional Parameters\n1. **paramList** (required): List of tensor handles representing model parameters\n2. **learningRate** (required): Learning rate (positive float)\n3. **weightDecay** (optional): Weight decay coefficient (default: 0.01)\n\n## Returns\n\nReturns a string handle that can be used to reference the optimizer in subsequent operations.\n\n## Description\n\nAdamW (Adam with Decoupled Weight Decay) is a variant of the Adam optimizer that fixes the weight decay implementation. Unlike standard Adam where weight decay is implemented as L2 regularization (which can interfere with the adaptive learning rate), AdamW applies weight decay directly to the parameters, decoupling it from the gradient-based update.\n\n**Key Differences from Adam:**\n- **Decoupled weight decay**: Applied directly to parameters, not through gradients\n- **Better generalization**: Often achieves better test performance than Adam\n- **Stable training**: More consistent convergence especially with weight decay\n- **Higher default weight decay**: Default 0.01 vs Adam's typical 0.0\n\n## See Also\n\n- [torch::optimizer_adam](optimizer_adam.md) - Adam optimizer (related algorithm)\n- [torch::optimizer_adamax](optimizer_adamax.md) - Adamax optimizer\n- [torch::optimizer_sgd](optimizer_sgd.md) - Stochastic Gradient Descent\n- [torch::optimizer_adagrad](optimizer_adagrad.md) - Adaptive gradient algorithm
